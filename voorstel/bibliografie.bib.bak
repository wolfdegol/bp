@Article{ZhongQiuZhao2019,
  author  = {Zhong-Qiu Zhao, Member, IEEE, Peng Zheng,Shou-tao Xu, and Xindong Wu, Fellow, IEEE},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  title   = {Object Detection with Deep Learning: A Review},
  year    = {2019},
}

@InProceedings{Karakottas2020,
  author    = {Karakottas, Antonis and Zioulis, Nikolaos and Doumanglou, Alexandros and Sterzentsenko, Vladimiros and Gkitsas, Vasileios and Zarpalas, Dimitrios and Daras, Petros},
  booktitle = {2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)},
  title     = {Xr360: A Toolkit for Mixed 360 and 3d Productions},
  year      = {2020},
  pages     = {1-6},
  doi       = {10.1109/ICMEW46912.2020.9105984},
  ranking   = {rank3},
  url       = {https://ieeexplore.ieee.org/abstract/document/9105984},
}

@Misc{Wang2023,
  author        = {Tianfu Wang and Menelaos Kanakis and Konrad Schindler and Luc Van Gool and Anton Obukhov},
  title         = {Breathing New Life into 3D Assets with Generative Repainting},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2309.08523},
  primaryclass  = {cs.CV},
  ranking       = {rank4},
}

@Misc{Raj2023,
  author        = {Amit Raj and Srinivas Kaza and Ben Poole and Michael Niemeyer and Nataniel Ruiz and Ben Mildenhall and Shiran Zada and Kfir Aberman and Michael Rubinstein and Jonathan Barron and Yuanzhen Li and Varun Jampani},
  title         = {DreamBooth3D: Subject-Driven Text-to-3D Generation},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {code niet publiek, text 2 3D image},
  eprint        = {2303.13508},
  primaryclass  = {cs.CV},
  ranking       = {rank2},
  urldate       = {2022-12-01},
}

@InProceedings{Xu2023,
  author    = {Xu, Dejia and Jiang, Yifan and Wang, Peihao and Fan, Zhiwen and Wang, Yi and Wang, Zhangyang},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {NeuralLift-360: Lifting an In-the-Wild 2D Photo to a 3D Object With 360deg Views},
  year      = {2023},
  month     = jun,
  pages     = {4479-4489},
  ranking   = {rank4},
  url       = {https://arxiv.org/abs/2211.16431},
}

@Book{Lindeberg2012,
  author       = {Lindeberg, Tony},
  title        = {Scale invariant feature transform},
  year         = {2012},
  note         = {QC 20120524},
  number       = {5},
  volume       = {7},
  abstractnote = {Scale Invariant Feature Transform (SIFT) is an image descriptor for image-based matching developed by David Lowe (1999,2004). This descriptor as well as related image descriptors are used for a large number of purposes in computer vision related to point matching between different views of a 3-D scene and view-based object recognition. The SIFT descriptor is invariant to translations, rotations and scaling transformations in the image domain and robust to moderate perspective transformations and illumination variations. Experimentally, the SIFT descriptor has been proven to be very useful in practice for robust image matching and object recognition under real-world conditions.In its original formulation, the SIFT descriptor comprised a method for detecting interest points from a grey-level image at which statistics of local gradient directions of image intensities were accumulated to give a summarizing description of the local image structures in a local neighbourhood around each interest point, with the intention that this descriptor should be used for matching corresponding interest points between different images. Later, the SIFT descriptor has also been applied at dense grids (dense SIFT) which have been shown to lead to better performance for tasks such as object categorization and texture classification. The SIFT descriptor has also been extended from grey-level to colour images and from 2-D spatial images to 2+1-D spatio-temporal video.},
  doi          = {10.4249/scholarpedia.10491},
  pages        = {10491},
  url          = {https://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-62443},
}

@Article{Tyagi2019,
  author = {Deepanshu Tyagi},
  title  = {Introduction to SIFT( Scale Invariant Feature Transform)},
  year   = {2019},
  url    = {https://medium.com/@deepanshut041/introduction-to-sift-scale-invariant-feature-transform-65d7f3a72d40},
}

@Article{Martinez2019,
  author    = {Martinez, KP and Untalan, MZG and Burgos, DFM and Ramos, RV and Germentil, MJQ},
  journal   = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title     = {Creation of a virtual reality environment of a university museum using 3D photogrammetric models},
  year      = {2019},
  pages     = {841--847},
  volume    = {42},
  publisher = {Copernicus Publications G{\"o}ttingen, Germany},
}

@InProceedings{Saxena2009,
  author    = {Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew Y.},
  booktitle = {2009 IEEE International Conference on Robotics and Automation},
  title     = {Learning 3-D object orientation from images},
  year      = {2009},
  pages     = {794-800},
  doi       = {10.1109/ROBOT.2009.5152855},
  ranking   = {rank4},
}

@Online{FormLabs,
  author  = {FormLabs},
  ranking = {rank3},
  title   = {Photogrammetry: Step-by-Step Guide and Software Comparison},
  url     = {https://formlabs.com/asia/blog/photogrammetry-guide-and-software-comparison/},
}

@Online{Microsoft,
  author  = {Microsoft},
  ranking = {rank3},
  title   = {Digital image-based modeling on Azur},
  url     = {https://learn.microsoft.com/en-us/azure/architecture/example-scenario/infrastructure/image-modeling},
}

@Online{3DFlow,
  author  = {3DFlow},
  ranking = {rank2},
  title   = {3DF Zephyr Documentation},
  url     = {https://www.3dflow.net/technology/documents/3df-zephyr-documentation/},
}

@Online{Kovko2023,
  author  = {Eugene Kovko},
  month   = dec,
  ranking = {rank4},
  title   = {How to Compare Two Algorithms Empirically?},
  url     = {https://www.baeldung.com/cs/compare-algorithms-performance},
  year    = {2023},
}

@Article{Mildenhall2020,
  author     = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  journal    = {CoRR},
  title      = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  year       = {2020},
  volume     = {abs/2003.08934},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2003-08934.bib},
  eprint     = {2003.08934},
  eprinttype = {arXiv},
  ranking    = {rank5},
  timestamp  = {Tue, 24 Mar 2020 16:42:29 +0100},
  url        = {https://arxiv.org/abs/2003.08934},
}

@Book{alma9993304067001488,
  author    = {Zhang, Yu-Jin},
  publisher = {Springer Nature Singapore},
  title     = {3-D Computer Vision: Principles, Algorithms and Applications},
  year      = {2023},
  address   = {Singapore},
  edition   = {1st ed. 2023.},
  isbn      = {981-19-7580-9},
  abstract  = {This textbook offers advanced content on computer vision (basic content can be found in its prerequisite textbook, “2D Computer Vision: Principles, Algorithms and Applications”), including the basic principles, typical methods and practical techniques. It is intended for graduate courses on related topics, e.g. Computer Vision, 3-D Computer Vision, Graphics, Artificial Intelligence, etc. The book is mainly based on my lecture notes for several undergraduate and graduate classes I have offered over the past several years, while a number of topics stem from my research publications co-authored with my students. This book takes into account the needs of learners with various professional backgrounds, as well as those of self-learners. Furthermore, it can be used as a reference guide for practitioners and professionals in related fields. To aid in comprehension, the book includes a wealth of self-test questions (with hints and answers). On the one hand, these questions help teachers to carry out online teaching and interact with students during lectures; on the other, self-learners can use them to assess whether they have grasped the key content.},
  booktitle = {3-D Computer Vision Principles, Algorithms and Applications},
  keywords  = {Computer vision},
  language  = {eng},
}

@Online{,
  title = {Neural Radiance Field (NeRF): A Gentle Introduction},
  url   = {https://datagen.tech/guides/synthetic-data/neural-radiance-field-nerf/#},
}

@Online{,
  title = {Marching Cubes},
  url   = {https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching cubes is a simple,a region of the function.},
}

@Repository{,
  abstract = {An awesome PyTorch NeRF library},
  title    = {NeRF-Factory: An awesome PyTorch NeRF collection},
  url      = {https://github.com/kakaobrain/NeRF-Factory},
}

@Comment{jabref-meta: databaseType:bibtex;}
