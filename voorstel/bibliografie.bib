@Article{ZhongQiuZhao2019,
  author  = {{Z}hong-{Q}iu {Z}hao, {M}ember, {IEEE}, {P}eng {Z}heng,{S}hou-tao {X}u, and {X}indong {W}u, {F}ellow, {IEEE}},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  title   = {{O}bject {D}etection with {D}eep {L}earning: {A} {R}eview},
  year    = {2019},
}

@Misc{Wang2023,
  author        = {Tianfu Wang and Menelaos Kanakis and Konrad Schindler and Luc Van Gool and Anton Obukhov},
  title         = {Breathing New Life into 3D Assets with Generative Repainting},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2309.08523},
  primaryclass  = {cs.CV},
  ranking       = {rank4},
}

@Misc{Raj2023,
  author        = {Amit Raj and Srinivas Kaza and Ben Poole and Michael Niemeyer and Nataniel Ruiz and Ben Mildenhall and Shiran Zada and Kfir Aberman and Michael Rubinstein and Jonathan Barron and Yuanzhen Li and Varun Jampani},
  title         = {DreamBooth3D: Subject-Driven Text-to-3D Generation},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {code niet publiek, text 2 3D image},
  eprint        = {2303.13508},
  primaryclass  = {cs.CV},
  ranking       = {rank2},
  urldate       = {2022-12-01},
}

@InProceedings{Xu2023,
  author    = {Xu, Dejia and Jiang, Yifan and Wang, Peihao and Fan, Zhiwen and Wang, Yi and Wang, Zhangyang},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {NeuralLift-360: Lifting an In-the-Wild 2D Photo to a 3D Object With 360deg Views},
  year      = {2023},
  month     = jun,
  pages     = {4479-4489},
  ranking   = {rank4},
  url       = {https://arxiv.org/abs/2211.16431},
}

@Book{Lindeberg2012,
  author       = {Lindeberg, Tony},
  title        = {Scale invariant feature transform},
  year         = {2012},
  note         = {QC 20120524},
  number       = {5},
  volume       = {7},
  abstractnote = {Scale Invariant Feature Transform (SIFT) is an image descriptor for image-based matching developed by David Lowe (1999,2004). This descriptor as well as related image descriptors are used for a large number of purposes in computer vision related to point matching between different views of a 3-D scene and view-based object recognition. The SIFT descriptor is invariant to translations, rotations and scaling transformations in the image domain and robust to moderate perspective transformations and illumination variations. Experimentally, the SIFT descriptor has been proven to be very useful in practice for robust image matching and object recognition under real-world conditions.In its original formulation, the SIFT descriptor comprised a method for detecting interest points from a grey-level image at which statistics of local gradient directions of image intensities were accumulated to give a summarizing description of the local image structures in a local neighbourhood around each interest point, with the intention that this descriptor should be used for matching corresponding interest points between different images. Later, the SIFT descriptor has also been applied at dense grids (dense SIFT) which have been shown to lead to better performance for tasks such as object categorization and texture classification. The SIFT descriptor has also been extended from grey-level to colour images and from 2-D spatial images to 2+1-D spatio-temporal video.},
  doi          = {10.4249/scholarpedia.10491},
  pages        = {10491},
  url          = {https://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-62443},
}

@Article{Tyagi2019,
  author = {Deepanshu Tyagi},
  title  = {Introduction to SIFT( Scale Invariant Feature Transform)},
  year   = {2019},
  url    = {https://medium.com/@deepanshut041/introduction-to-sift-scale-invariant-feature-transform-65d7f3a72d40},
}

@Article{Martinez2019,
  author    = {Martinez, KP and Untalan, MZG and Burgos, DFM and Ramos, RV and Germentil, MJQ},
  journal   = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title     = {Creation of a virtual reality environment of a university museum using 3D photogrammetric models},
  year      = {2019},
  pages     = {841--847},
  volume    = {42},
  publisher = {Copernicus Publications G{\"o}ttingen, Germany},
}

@InProceedings{Saxena2009,
  author    = {Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew Y.},
  booktitle = {2009 IEEE International Conference on Robotics and Automation},
  title     = {Learning 3-D object orientation from images},
  year      = {2009},
  pages     = {794-800},
  doi       = {10.1109/ROBOT.2009.5152855},
  ranking   = {rank4},
}

@Online{FormLabs,
  author  = {FormLabs},
  ranking = {rank3},
  title   = {Photogrammetry: Step-by-Step Guide and Software Comparison},
  url     = {https://formlabs.com/asia/blog/photogrammetry-guide-and-software-comparison/},
}

@Online{Microsoft,
  author  = {Microsoft},
  ranking = {rank3},
  title   = {Digital image-based modeling on Azur},
  url     = {https://learn.microsoft.com/en-us/azure/architecture/example-scenario/infrastructure/image-modeling},
}

@Online{3DFlow,
  author  = {3DFlow},
  ranking = {rank2},
  title   = {3DF Zephyr Documentation},
  url     = {https://www.3dflow.net/technology/documents/3df-zephyr-documentation/},
}

@Online{Kovko2023,
  author  = {Eugene Kovko},
  month   = dec,
  ranking = {rank4},
  title   = {How to Compare Two Algorithms Empirically?},
  url     = {https://www.baeldung.com/cs/compare-algorithms-performance},
  year    = {2023},
}

@Article{Mildenhall2020,
  author     = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  journal    = {CoRR},
  title      = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  year       = {2020},
  volume     = {abs/2003.08934},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2003-08934.bib},
  eprint     = {2003.08934},
  eprinttype = {arxiv},
  ranking    = {rank5},
  timestamp  = {Tue, 24 Mar 2020 16:42:29 +0100},
}

@Book{Zhang2023,
  author    = {Zhang, Yu-Jin},
  publisher = {Springer Nature Singapore},
  title     = {3-D Computer Vision: Principles, Algorithms and Applications},
  year      = {2023},
  address   = {Singapore},
  edition   = {1st ed. 2023.},
  isbn      = {981-19-7580-9},
  abstract  = {This textbook offers advanced content on computer vision (basic content can be found in its prerequisite textbook, “2D Computer Vision: Principles, Algorithms and Applications”), including the basic principles, typical methods and practical techniques. It is intended for graduate courses on related topics, e.g. Computer Vision, 3-D Computer Vision, Graphics, Artificial Intelligence, etc. The book is mainly based on my lecture notes for several undergraduate and graduate classes I have offered over the past several years, while a number of topics stem from my research publications co-authored with my students. This book takes into account the needs of learners with various professional backgrounds, as well as those of self-learners. Furthermore, it can be used as a reference guide for practitioners and professionals in related fields. To aid in comprehension, the book includes a wealth of self-test questions (with hints and answers). On the one hand, these questions help teachers to carry out online teaching and interact with students during lectures; on the other, self-learners can use them to assess whether they have grasped the key content.},
  booktitle = {3-D Computer Vision Principles, Algorithms and Applications},
  keywords  = {Computer vision},
  language  = {eng},
}

@Online{Fisher2014,
  author = {Matthew Fisher},
  title  = {Marching Cubes},
  url    = {https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching cubes is a simple,a region of the function.},
  year   = {2014},
}

@Repository{Jeong,
  abstract = {An awesome PyTorch NeRF library},
  author   = {Yoonwoo Jeong, Seungjoo Shin, Kibaek Park.},
  title    = {NeRF-Factory: An awesome PyTorch NeRF collection},
  url      = {https://github.com/kakaobrain/NeRF-Factory},
}

@InProceedings{Schonberger2016,
  author    = {Schonberger, Johannes L. and Frahm, Jan-Michael},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Structure-From-Motion Revisited},
  year      = {2016},
  month     = jun,
}

@Article{Mueller2022,
  author     = {Thomas M\"uller and Alex Evans and Christoph Schied and Alexander Keller},
  journal    = {ACM Trans. Graph.},
  title      = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  year       = {2022},
  month      = jul,
  number     = {4},
  pages      = {102:1--102:15},
  volume     = {41},
  address    = {New York, NY, USA},
  articleno  = {102},
  doi        = {10.1145/3528223.3530127},
  issue_date = {July 2022},
  numpages   = {15},
  publisher  = {ACM},
  url        = {https://doi.org/10.1145/3528223.3530127},
}

@Book{Luebke2002,
  author    = {Luebke, David and Reddy, Martin and Cohen, Jonathan and Varshney, Amitabh and Watson, Benjamin and Huebner, Robert},
  publisher = {Morgan Kaufmann},
  title     = {Level of detail for 3D graphics},
  year      = {2002},
  edition   = {1st edition},
  isbn      = {1-55860-838-9},
  abstract  = {Level of detail (LOD) techniques are increasingly used by professional real-time developers to strike the balance between breathtaking virtual worlds and smooth, flowing animation. Level of Detail for 3D Graphics brings together, for the first time, the mechanisms, principles, practices, and theory needed by every graphics developer seeking to apply LOD methods. Continuing advances in level of detail management have brought this powerful technology to the forefront of 3D graphics optimization research. This book, written by the very researchers and developers who have built LOD technology, is both a state-of-the-art chronicle of LOD advances and a practical sourcebook, which will enable graphics developers from all disciplines to apply these formidable techniques to their own work. * Is a complete, practical resource for programmers wishing to incorporate LOD technology into their own systems. * Is an important reference for professionals in game development, computer animation, information visualization, real-time graphics and simulation, data capture and preview, CAD display, and virtual worlds. * Is accessible to anyone familiar with the essentials of computer science and interactive computer graphics. * Covers the full range of LOD methods from mesh simplification to error metrics, as well as advanced issues of human perception, temporal detail, and visual fidelity measurement. * Includes an accompanying Web site rich in supplementary material including source code, tools, 3D models, public domain software, documentation, LOD updates, and more. Visit http://LODBook.com.},
  keywords  = {Computer graphics},
  language  = {eng},
}

@Article{NeRFshop23,
  author  = {Jambon, Cl\'ement and Kerbl, Bernhard and Kopanas, Georgios and Diolatzis, Stavros and Leimk{\"u}hler, Thomas and Drettakis, George"},
  journal = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
  title   = {NeRFshop: Interactive Editing of Neural Radiance Fields"},
  year    = {2023},
  month   = may,
  number  = {1},
  volume  = {6},
  url     = {https://repo-sam.inria.fr/fungraph/nerfshop/},
}

@Conference{Tancik2023,
  author = {Matthew Tancik},
  title  = {The Metaverse: What, How, Why and When},
  year   = {20},
  url    = {https://www.youtube.com/watch?v=isKbsNKArJU},
}

@InProceedings{Tancik2023a,
  author    = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and McAllister, David and Kanazawa, Angjoo},
  booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
  title     = {Nerfstudio: A Modular Framework for Neural Radiance Field Development},
  year      = {2023},
  series    = {SIGGRAPH '23},
}

@Article{Wang2023a,
  author    = {Wang, Dongqing and Zhang, Tong and Abboud, Alaa and Süsstrunk, Sabine},
  journal   = {arXiv.org},
  title     = {InpaintNeRF360: Text-Guided 3D Inpainting on Unbounded Neural Radiance Fields},
  year      = {2023},
  issn      = {2331-8422},
  abstract  = {Neural Radiance Fields (NeRF) can generate highly realistic novel views. However, editing 3D scenes represented by NeRF across 360-degree views, particularly removing objects while preserving geometric and photometric consistency, remains a challenging problem due to NeRF's implicit scene representation. In this paper, we propose InpaintNeRF360, a unified framework that utilizes natural language instructions as guidance for inpainting NeRF-based 3D scenes.Our approach employs a promptable segmentation model by generating multi-modal prompts from the encoded text for multiview segmentation. We apply depth-space warping to enforce viewing consistency in the segmentations, and further refine the inpainted NeRF model using perceptual priors to ensure visual plausibility. InpaintNeRF360 is capable of simultaneously removing multiple objects or modifying object appearance based on text instructions while synthesizing 3D viewing-consistent and photo-realistic inpainting. Through extensive experiments on both unbounded and frontal-facing scenes trained through NeRF, we demonstrate the effectiveness of our approach and showcase its potential to enhance the editability of implicit radiance fields.},
  address   = {Ithaca},
  copyright = {2023. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
}

@Article{Kerbl2023,
  author  = {Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
  journal = {ACM Transactions on Graphics},
  title   = {3D Gaussian Splatting for Real-Time Radiance Field Rendering},
  year    = {2023},
  month   = jul,
  number  = {4},
  volume  = {42},
  url     = {https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/},
}

@Comment{jabref-meta: databaseType:bibtex;}
